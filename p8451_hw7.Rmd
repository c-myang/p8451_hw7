---
title: "Machine Learning for Epi: Assignment 7"
output:
  html_document: default
  word_document: default
date: "2023-03-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = F,
                      message = F,
                      fig.dim = c(12, 7))

library(tidyverse)
library(randomForest)
library(caret)
library(gbm)
library(pROC)
library(rpart.plot)
library(rpart)
library(gtsummary)
```

## Description of Data

The data we will be using are from data analyzed by Mattheny et al. 2021, a patient dataset of all inpatients discharged from Vanderbilt University Medical Center between January 1, 2007, and December 31, 2016, with a primary diagnosis of AMI. *We will be using this dataset to predict readmission for myocardial infarction.* We have restricted the dataset to 14 features and an outcome which distinguishes those who were readmitted to the hospital for MI within 30 days of discharge (1=Yes, 0=No).

## Analysis Pipeline

The pipeline for this analysis is outlined below:

1. Data Preparation
  -	Removing ID variable
  -	Removing missing variables
  -	Converting variables to factor when appropriate
  -	Check balance
  -	Centering and scaling (important for looking at variable importance)

2. Data partitioning
  -	70/30 split into training and testing data

3. Training
  -	5-fold cross validation on the full training set
  - Train best model based on area under the receiver operating characteristic curve (AUROC) performance metric
  -	Downsample data if unbalanced
  -	Tuning hyperparameters: 
    - Elastic Net: alpha and lambda
    - Classification Tree: Cp
    -	Random Forest: mtry
    
4. Comparing model performance
  -	Compare AUROC, sensitivity, and specificity across models
  - Select model based on AUROC

5. Apply model to testing data
  -	Evaluate confusion matrix
  - Examine variable importance if applicable

### Step 1: Load data and prepare for analysis

The code chunk below loads the data, omits missing observations, and converts the variables Race1, Education, HHIncome, Diabetes, PhysActive, and Smoke100 to factor variables. Although Classification Trees can handle missing data, SVC models cannot, and therefore we will omit missing variables to be able to compare model performance on the same subset of training data.

```{r load_data}
mi_data = readr::read_csv("./mi.data.csv") %>% 
  mutate(across(.cols = c(Pulm.adema, Arr:readmission), ~ ifelse(. == 1, "Yes", "No"))) %>% 
  mutate(Sex = case_when(Sex == 0 ~ "Male", 
                         Sex == 1 ~ "Female", 
                         Sex == 2 ~ "Non-binary")) %>% 
  mutate(FC = case_when(FC == 0 ~ "None", 
                         FC == 1 ~ "I", 
                         FC == 2 ~ "II",
                         FC == 3 ~ "III",
                         FC == 4 ~ "IV")) %>% 
  mutate(Sex = as.factor(Sex), 
         Pulm.adema = as.factor(Pulm.adema),
         FC = as.factor(FC), 
         Arr = as.factor(Arr), 
         Diab = as.factor(Diab), 
         Obesity = as.factor(Obesity),
         Asthma = as.factor(Asthma),
         readmission = as.factor(readmission)) %>% 
  select(-ID) 


# Center and scale mi_data
set.up.preprocess = preProcess(mi_data, method = c("center", "scale"))
mi_data = predict(set.up.preprocess, mi_data)

summary(mi_data)

```

Our resulting dataset contains 1700 observations of 15 features, with 1 containing our binary outcome variable, `readmission`. Based on the summary, we can see that the distribution of MI readmits is quite unbalanced, with a 9.35% prevalence of cases. 

### Step 2: Partition the data 

The code chunk below partitions the data into training and testing sets, using a 70/30 split. 

```{r partition_data}
set.seed(123)

#Creating balanced partitions in the data
train_index = createDataPartition(mi_data$readmission, p = 0.7, list = FALSE)

mi_train = mi_data[train_index,]
mi_test = mi_data[-train_index,]

#Check distribution of the outcome between train and test data
summary(mi_train$readmission) 
summary(mi_test$readmission)
```

We can see that there are similar distributions of the variable `readmission`, with approximately 9% of observations having having to be readmitted across both the training and testing sets, indicating that the data were successfully partitioned.

### Step 3: Model Fitting

We will fit 3 prediction models to predict readmission for MI. (feature name: `readmission``). 

- Model 1 (`class_tree`): Classification Tree based on all features.

- Model 2 (`mod_rf`): Random Forest based on all features.

- Model 3 (`mod_EN`): A logistic model based on all features that will serve as our baseline model.

The models will be trained and selected based on the highest Area Under the ROC (AUROC) rather than accuracy, because we want to balance the sensitivity and specificity for readmission classification. This will be done using `summaryFunction = twoClassSummary` and `metric = "ROC"` options within `trainControl()` and `train()` in caret, respectively.

#### Classification Tree

To fit the classification tree, we will tune the complexity hyperparameter using a tune grid of values ranging from 0.01 to 0.04, searching in increments of 0.001. We will use down-sampling because of a 90/10 imbalance of the outcome variable in the data. Moreover, given the small number of outcomes in the training data, we will reduce the number of cross-validation folds to 5.

```{r classtree}
set.seed(123)

#Creating 10-fold cross-validation and using down-sampling because of imbalance in data
train.control.class = trainControl(method = "cv", number = 5, sampling = "down",
                                   summaryFunction = twoClassSummary, classProbs = TRUE)

#Create sequence of cp parameters to try 
grid.2 = expand.grid(cp = seq(0.01, 0.04, by = 0.001))

#Train model
class_tree = train(readmission ~ ., data = mi_train, method = "rpart", trControl = train.control.class, tuneGrid = grid.2, metric = "ROC")

# View results
class_perf = class_tree$results %>% arrange(desc(ROC)) %>% head() 
class_perf %>% knitr::kable()

# Obtain variable importance on the final model within training data
varImp(class_tree)

# Plot classification tree
rpart.plot(class_tree$finalModel)

# Save results
class_perf = class_perf %>% slice(1) %>% select(-cp)
```

The resulting model found that for the most optimal AUROC of 54.03%, there is a relatively small complexity parameter Cp of 0.026, which suggests that increasing the size and complexity of the tree yields better model performance. We can see that the most important variables in the classification tree in descending order of importance include WBC, SBP, and sodium, and as such these are the features that are split near the top of the tree, while lower-importance variables are split near the bottom of the tree.

#### Random Forest Model

To fit the Random Forest model, we will train using 5-fold cross-validation, and set the tune grid for the mtry hyperparameter to sample anywhere from 1 to 14 (all) variables in data. We will also use down-sampling because of a 90/10 imbalance of the outcome variable in the data. We will set the number of trees to 150. 

```{r mod_rf}
set.seed(123)

#Set 5-fold cross-validation and downsample
train_control = trainControl(method = "cv", number = 5, sampling = "down", 
                             classProbs = T, summaryFunction = twoClassSummary)

#Trying grid of values of mtry
mtry.grid = expand.grid(.mtry = seq(1, 14, by = 1))

mod_rf = train(readmission ~ ., data = mi_train, method = "rf", metric = "ROC", tuneGrid = mtry.grid,
               trControl = train_control, 
               ntree = 200)

plot(varImp(mod_rf))

# View results
rf_perf = mod_rf$results %>% arrange(desc(ROC)) %>% head() 
rf_perf %>% knitr::kable()

# Save results
rf_perf = rf_perf %>% 
  slice(1) %>%
  select(-mtry)
```

The resulting model found that for the highest model performance AUROC of 58.83%, the optimal mtry hyperparameter value was 1, which suggests that random sampling single variable to split for each tree was sufficient for high model performance. We can see that the most important variables in the random forest in descending order of importance include WBC, SBP, and Age, which are similar to the results we find in the classification tree, where WBC and SBP were the most important variables.

#### Elastic Net Regression Model

To fit the elastic model, we will feed all features into the model, and train within caret using 5-fold CV and down-sampling. We will set the tune length to 100 for the number of combinations to search for alpha and lambda.

```{r mod_EN}
set.seed(123)

train_control = trainControl(method = "cv", number = 5, sampling = "down", classProbs = T, summaryFunction = twoClassSummary)

mod_EN = train(readmission ~ ., data = mi_train, method = "glmnet", 
               trControl = train_control,
               tuneLength = 100)

#Print the values of alpha and lambda that gave best prediction
mod_EN$bestTune

# View results
EN_perf = mod_EN$results %>% arrange(desc(ROC)) %>% head() 
EN_perf %>% knitr::kable()

# Save results
EN_perf = EN_perf %>% 
  slice(1) %>%
  select(-alpha, -lambda)
```

The resulting model yielded an AUROC of 62.385%, with hyperparameters of alpha = 0.264 and lambda = 0.0298.

#### Comparing performance across models

Finally, let's compare the performance results on the training data across the 3 models.

```{r compare}
rbind(class_perf, rf_perf, EN_perf) %>% 
  mutate(Model = c("Classification Tree", "Random Forest", "Elastic Net")) %>% 
  relocate(Model) %>% 
  arrange(desc(ROC)) %>% 
  knitr::kable(digits = 4)
```

The table shows that the Elastic Net model has the best performance as measured by AUROC (62.38%%), followed by the random forest model (60.55%), then the Classification Tree (54.03%). We can see that for the Elastic Net model, there is a better balance of sensitivity (55.41%) and specificity (65.22%) compared to the random forest and classification tree models, which have lower sensitivities. Having a parametric model with the best performance may indicate that the relationships in our data did not have sufficient complexity to warrant implementing a more complex ensemble method, and lead to an improved performance of the Elastic Net model versus the tree-based models. Therefore, based on interpretability and model performance I would choose the Elastic Net as my final model to optimally classify MI readmission.

### Step 4: Final Model Evaluation

Finally, we will evaluate the performance our final Elastic Net model by making predictions in the test data. We will use the `confusionMatrix()` function to get performance measures of sensitivity, specificity, NPV, and PPV, and accuracy for the model, and plot the ROC curve.

```{r test}
# Make predictions in test set
pred = mod_EN %>% predict(mi_test)
mi_test = mi_test %>% mutate(pred = as.factor(pred))

# Get evaluation metrics from test set
cm = confusionMatrix(data = mi_test$pred, reference = mi_test$readmission, positive = "Yes")

#Create ROC Curve for Analysis
pred_prob <- predict(mod_EN, mi_test, type = "prob")

# Plot Area under the Receiver Operating Curve (AUROC)
analysis =  roc(response = mi_test$readmission, predictor = pred_prob[,2])

# View results
cm
analysis$auc

plot(1 - analysis$specificities, analysis$sensitivities, type = "l",
ylab = "Sensitivity",xlab = "1-Specificity", col = "black", lwd = 2,
main = "ROC Curve for Diabetes Classification")
abline(a = 0, b = 1)
```

On the testing set, we can see that the AUROC of our final model is 59.72% (95% CI : 0.5037, 0.592), with an accuracy of 54.81%, sensitivity of 61.7%, and specificity of 54.1%. The plot shows an ROC curve that is close to the diagonal line, suggesting that the performance of our model is only minimally improved over a random classifier. Moreover, we see a large imbalance between the PPV (12.03%) and NPV (93.28%), which may be affected due to low prevalence of readmission cases in the data such that there is a much higher probability of truly detecting persons without readmission. 
