---
title: "Machine Learning for Epi: Assignment 7"
output:
  html_document: default
  word_document: default
date: "2023-03-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = F,
                      message = F,
                      fig.dim = c(12, 7))

library(tidyverse)
library(randomForest)
library(caret)
library(gbm)
library(pROC)
library(rpart.plot)
library(rpart)
```

## Description of Data

The data we will be using are from data analyzed by Mattheny et al. 2021, a patient dataset of all inpatients discharged from Vanderbilt University Medical Center between January 1, 2007, and December 31, 2016, with a primary diagnosis of AMI. *We will be using this dataset to predict readmission for myocardial infarction.* We have restricted the dataset to 14 features and an outcome which distinguishes those who were readmitted to the hospital for MI within 30 days of discharge (1=Yes, 0=No).

### Step 1: Load data and prepare for analysis

The code chunk below loads the data, omits missing observations, and converts the variables Race1, Education, HHIncome, Diabetes, PhysActive, and Smoke100 to factor variables. Although Classification Trees can handle missing data, SVC models cannot, and therefore we will omit missing variables to be able to compare model performance on the same subset of training data.

```{r load_data}
mi_data = readr::read_csv("./mi.data.csv") %>% 
  mutate(across(.cols = c(Pulm.adema, Arr:readmission), ~ ifelse(. == 1, "Yes", "No"))) %>% 
  mutate(Sex = case_when(Sex == 0 ~ "Male", 
                         Sex == 1 ~ "Female", 
                         Sex == 2 ~ "Non-binary")) %>% 
  mutate(FC = case_when(FC == 0 ~ "None", 
                         FC == 1 ~ "I", 
                         FC == 2 ~ "II",
                         FC == 3 ~ "III",
                         FC == 4 ~ "IV")) %>% 
  mutate(Sex = as.factor(Sex), 
         Pulm.adema = as.factor(Pulm.adema),
         FC = as.factor(FC), 
         Arr = as.factor(Arr), 
         Diab = as.factor(Diab), 
         Obesity = as.factor(Obesity),
         Asthma = as.factor(Asthma),
         readmission = as.factor(readmission)) %>% 
  select(-ID) 

summary(mi_data)

```

Our resulting dataset contains 1700 observations of 15 features, with 1 containing our binary outcome variable, `readmission`. Based on the summary, we can see that the distribution of MI readmits is quite unbalanced, with a 9.35% prevalence of cases. 

### Step 2: Partition the data 

The code chunk below partitions the data into training and testing sets, using a 70/30 split. 

```{r partition_data}
set.seed(123)

#Creating balanced partitions in the data
train_index = createDataPartition(mi_data$readmission, p = 0.7, list = FALSE)

mi_train = mi_data[train_index,]
mi_test = mi_data[-train_index,]

#Check distribution of the outcome between train and test data
summary(mi_train$readmission) 
summary(mi_test$readmission)

```

We can see that there are similar distributions of the variable `readmission`, with approximately 9% of observations having having to be readmitted across both the training and testing sets, indicating that the data were successfully partitioned.

### Step 3: Model Fitting

We will fit 3 prediction models to predict readmission for MI. (feature name: `readmission``). 

- Model 1 (`class_tree`): Classification Tree based on all features.

- Model 2 (`mod_rf`): Random Forest based on all features.

- Model 3 (`mod_EN`): A logistic model based on all features that will serve as our baseline model.

The models will be trained and selected based on the highest Area Under the ROC (AUROC) rather than accuracy, because we want to balance the sensitivity and specificity for readmission classification. This will be done using `summaryFunction = twoClassSummary` and `metric = "ROC"` options within `trainControl()` and `train()` in caret, respectively.

#### Classification Tree

To fit the classification tree, we will tune the complexity hyperparameter using a tune grid of values ranging from 0.01 to 0.04, searching in increments of 0.001. We will use down-sampling because of a 90/10 imbalance of the outcome variable in the data. Moreover, given the small number of outcomes in the training data, we will reduce the number of cross-validation folds to 5.

```{r classtree}
set.seed(123)

#Creating 10-fold cross-validation and using down-sampling because of imbalance in data
train.control.class = trainControl(method = "cv", number = 5, sampling = "down",
                                   summaryFunction = twoClassSummary, classProbs = TRUE)

#Create sequence of cp parameters to try 
grid.2 = expand.grid(cp = seq(0.01, 0.04, by = 0.001))

#Train model
class_tree = train(readmission ~ ., data = mi_train, method = "rpart", trControl = train.control.class, tuneGrid = grid.2, metric = "ROC")

# View results
class_perf = class_tree$results %>% arrange(desc(ROC)) %>% head() 
class_perf %>% knitr::kable()

# Obtain variable importance on the final model within training data
varImp(class_tree)

# Plot classification tree
rpart.plot(class_tree$finalModel)

# Save results
class_perf = class_perf %>% slice(1) %>% select(-cp)
```

The resulting model found that for the most optimal AUROC of 54.03%, there is a relatively small complexity parameter Cp of 0.026, which suggests that increasing the size and complexity of the tree yields better model performance. We can see that the most important variables in the classification tree in descending order of importance include WBC, SBP, and sodium, and as such these are the features that are split near the top of the tree, while lower-importance variables are split near the bottom of the tree.

#### Random Forest Model

To fit the Random Forest model, we will train using 5-fold cross-validation, and set the tune grid for the mtry hyperparameter to values ranging from 2 to 3 in increments of 0.01. We will also use down-sampling because of a 90/10 imbalance of the outcome variable in the data.

```{r mod_rf}
set.seed(123)

#Set 5-fold cross-validation and downsample
train_control = trainControl(method = "cv", number = 5, sampling = "down", 
                             classProbs = T, summaryFunction = twoClassSummary)

#Trying three different values of mtry
mtry.vals = c(ncol(mi_train)-1, sqrt(ncol(mi_train)-1), 0.5*ncol(mi_train)-1)
mtry.grid = expand.grid(.mtry=round(mtry.vals))
mtry.grid = expand.grid(.mtry = seq(2, 3, by = 0.01))

mod_rf = train(readmission ~ ., data = mi_train, method = "rf", metric = "ROC", tuneGrid = mtry.grid,
               trControl = train_control, 
               ntree = 100)

confusionMatrix(mod_rf)
mod_rf$results
mod_rf$bestTune
mod_rf$finalModel

varImp(mod_rf)
plot(varImp(mod_rf))

varImpPlot(mod_rf$finalModel)

# View results
rf_perf = mod_rf$results %>% arrange(desc(ROC)) %>% head() 
rf_perf %>% knitr::kable()

# Save results
rf_perf = rf_perf %>% 
  slice(1) %>%
  select(-mtry)
```

For the most optimal AUROC of 81.63%, the the trained SVC model found a hyperparameter (C) of 1.035, allowing a moderate threshold of misclassification between classes. As confirmed by the plot, other values of C from 0.001 to 2 did not optimize the ROC. 

#### Elastic Net Regression Model

To fit the logistic model, we will feed all features into the model, and train within caret on the training dataset.

```{r mod_EN}
set.seed(123)

train_control = trainControl(method = "cv", number = 5, sampling = "down", classProbs = T, summaryFunction = twoClassSummary)

mod_EN = train(readmission ~ ., data = mi_train, method = "glmnet", 
                 trControl = train_control, 
                 tuneLength = 100)

#Print the values of alpha and lambda that gave best prediction
mod_EN$bestTune

# View results
EN_perf = mod_EN$results %>% arrange(desc(ROC)) %>% head() 
EN_perf %>% knitr::kable()

# Save results
EN_perf = EN_perf %>% 
  slice(1) %>%
  select(-alpha, -lambda)
```

The resulting model yielded an AUROC of 62.385%.

#### Comparing performance across models

Finally, let's compare the performance results on the training data across the 3 models.

```{r compare}
rbind(class_perf, rf_perf, EN_perf) %>% 
  mutate(Model = c("Classification Tree", "Random Forest", "Elastic Net")) %>% 
  relocate(Model) %>% 
  arrange(desc(ROC)) %>% 
  knitr::kable(digits = 4)
```

The table shows that the Elastic Net model has the best performance as measured by AUROC (62.38%%), followed by the random forest model (60.55%), then the Classification Tree (54.03%). We can see that for the Elastic Net model, there is a better balance of sensitivity (55.41%) and specificity (65.22%) compared to the random forest and classification tree models, which have lower sensitivities. Therefore, I would choose the Elastic Net as my final model to optimally classify MI readmission.

### Step 4: Final Model Evaluation

Finally, we will evaluate the performance our final SVC model by making predictions in the test data. We will use the `confusionMatrix()` function to get performance measures of sensitivity, specificity, NPV, and PPV, and accuracy for the model, and plot the ROC curve.

```{r test}
# Make predictions in test set
pred = mod_EN %>% predict(mi_test)
mi_test = mi_test %>% mutate(pred = as.factor(pred))

# Get evaluation metrics from test set
cm = confusionMatrix(data = mi_test$pred, reference = mi_test$readmission, positive = "Yes")

#Create ROC Curve for Analysis
pred_prob <- predict(mod_EN, mi_test, type = "prob")

# Plot Area under the Receiver Operating Curve (AUROC)
analysis =  roc(response = mi_test$readmission, predictor = pred_prob[,2])

# View results
cm
analysis$auc

plot(1 - analysis$specificities, analysis$sensitivities, type = "l",
ylab = "Sensitivity",xlab = "1-Specificity", col = "black", lwd = 2,
main = "ROC Curve for Diabetes Classification")
abline(a = 0, b = 1)
```

On the testing set, we can see that the AUROC of our final model is 59.72%, with accuracy of 54.81%, sensitivity of 61.7%, and specificity of 54.1%. Moreover, we see a large imbalance between the PPV (12.03%) and NPV (93.28%), which may be affected due to low prevalence of readmission cases in the data such that there is a much higher probability of truly detecting persons without readmission.

### Elastic Net Model Limitations

One main consideration that arose was the imbalance of cases and controls in the NHANES data that we used to train the model on, where there were a disproportionate number of cases without diabetes compared to those with diabetes. With imbalanced data, information required to make an accurate prediction about the minority class is limited, and therefore when applied to new data, the model may not perform well when trying to predict new cases of diabetes. Although we dealt with this issue by downsampling while training our model, we saw a lower PPV, and this limitation is what we may expect when applying the algorithm on new data with a low prevalence of diabetes.

Another limitation of the SVC model is that it does not inherently perform feature selection, and therefore may result in limited prediction performance and overfitting on new datasets because of irrelevant features that were included in the model when training on the original data. As such, there can be limitations when applying this model on large datasets or high-dimensional feature spaces. Moreover, depending on what types of applications the SVC model is used for, there are also limitations to the model's interpretability.

